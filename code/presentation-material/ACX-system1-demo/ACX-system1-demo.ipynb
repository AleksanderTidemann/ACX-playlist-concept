{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACX Dataset \n",
    "\n",
    "The dataset is the \"Ground Truth\" of this ACX Music system, its encyclopedia. Every time we want to get tracks or generate playlists, the dataset will be used as a reference point. The system is also built around a fixed amount of emotional categories. These categories are used to annotate the tracks in the dataset, by the user to rate tracks, and finally when we request playlists. The avaliable parameters are:\n",
    " \n",
    "- amazement\n",
    "- solemnity\n",
    "- tenderness\n",
    "- nostalgia \n",
    "- calmness\n",
    "- power\n",
    "- joyful_activation\n",
    "- tension\n",
    "- sadness\n",
    "\n",
    "Let us first consider what our data actually looks like. Note how every track has an associated emotional annotation, average age and mood.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data.jpg\" alt=\"audio features\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aljanaki, A., Wiering, F., & Veltkamp, R. C. (2016). Studying emotion inducedby music through a crowdsourcing game. ,52(1), 115â€“128.  doi:  10.1016/j.ipm.2015.03.004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Feature Extraction \n",
    "For this sytem to work, we need extract and add audio features from the audio associated with each ```track id``` to our dataset. Let us, for now, simply add some random numbers to each row to illustrate how these audio features would be added and what it would look like in the dataset.\n",
    "\n",
    "Why this process is necessary will become apparant later on in this demonstration.\n",
    "\n",
    "<img src=\"spoof_features_with_emo - Copy.jpg\" alt=\"audio features\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets dive inn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ACXmusic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a user\n",
    "Here we the users name and age. More information can also be logged here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "user1 = ACXmusic.User('aleks', 'tidemann', 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Then we log in\n",
    "When we log in, we store general emotional information about the persons current state. In this prototype, we only log the users current ```mood``` (from bad (1) to good (5)) as well as the ```time of day``` and total number of ```log ins```.\n",
    "\n",
    "Which parameters to log when the logging in can be adjusted to fit whatever ACX wants to track or use for playlist generation. How this ```mood``` parameter is used in this system will soon be apparant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "user1.log_in(4) #Current user mood (from 1-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Info\n",
    "Let us check some general user info to see how many times the user has used the app in the past and how many tracks is the user currently has rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Full name - aleks tidemann\\nAge - 28\\nLogins - 3\\nTracks rated - 4'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user1.user_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request a playlist\n",
    "Now we ready to start the process of *requesting a playlist* based on the users current emtional state and his/hers desired emotional state. These states are referd to as ```From state``` and ```To state```. To generate a playlist, we first need to find tracks that correspond to our ```From state``` and to our ```To state```. Then, these tracks will be used to generate an actual playlist later on.\n",
    "\n",
    "Let us start the process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_tracks_matchCheck --> No 'from_state' matches found in user profile. Searching external database..\n",
      "get_tracks_matchCheck_get_database_track --> More than one match found in the external database. Checking track associated moods to determine closest match.\n",
      "\n",
      "get_tracks_matchCheck --> No 'to_state' matches found in user profile. Searching external database..\n",
      "get_tracks_matchCheck_get_database_track --> One match found in external database.\n",
      "\n",
      "Done!\n",
      "Our ['sadness'] tune is track nr.176 in the dataset\n",
      "Our ['amazement', 'joyful_activation'] tune is track nr.148 in the dataset\n"
     ]
    }
   ],
   "source": [
    "from_state = [\"sadness\"]\n",
    "to_state = [\"amazement\", \"joyful_activation\"]\n",
    "\n",
    "tracks = user1.get_tracks(from_state, to_state)\n",
    "print(\"Done!\")\n",
    "print(\"Our {} tune is track nr.{} in the dataset\".format(from_state, tracks[0]))\n",
    "print(\"Our {} tune is track nr.{} in the dataset\".format(to_state, tracks[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is actually happening here?\n",
    "\n",
    "Why do we get these tracks as output?\n",
    "Well, first we find all tracks in the **user profile** and compare them to the from/to state.\n",
    "* If we find only ONE match, return it and we are done.\n",
    "* If we find MORE than one match, we check the current ```mood``` of the user and return the track whose log-in mood corresponds best.\n",
    "\n",
    "If we find NO matches in the user profile, we check the entire **external database** for matches:\n",
    "* If we find only ONE match in the database, return it and we are done.\n",
    "* If we find MORE than one match, we check the current mood of the user and return the track which mood corresponds best.\n",
    "\n",
    "If we find NO matches in the database either, we check the **hamming distance** between the desired state and EVERY track in the database.\n",
    "* If we find only ONE match in the database, return it and we are done.\n",
    "* If more then one match is found, we again look at the users current ```mood``` to decide which track is closest.\n",
    "\n",
    "It is also very much possible to not \"travel\" from one state to another. Simply pass the same emotional category in both states. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Vector Interpolation\n",
    "Next we would pass the \"tracks\" variable to a method called ```linear vector interpolation```. The method would first look for the tracks associated **audio features** in the dataset, as discussed previously and as shown in the illustration below.\n",
    "\n",
    "<img src=\"spoof_features_with_emo%20-%20Copy.jpg\" alt=\"audio features\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, this method would **interpolate** between these audio features (vectors). The illustration below imagines a scenario where the user wants a playlist with 4 tracks. The illustration also only consideres 4 audio features instead of over 200 as in our actual dataset. Every red circle represents a track, a step along a high-dimensional \"journey\" twoards the users desired emotional state. \n",
    "\n",
    "<img src=\"linear-interpolation-w-imaginary-tracks.jpg\" alt=\"Linear\" width=\"600\"/>\n",
    "\n",
    "Effectively, these \"steps\" can be thought of as *imaginary tracks* because they have a collection of audio features which most likely doesnt correspond to any \"real\" audio file. \n",
    "\n",
    "\n",
    "But **WHY?!**\n",
    "Let me explain.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "Lastly, the *imaginary tracks* would be sent to a regressor algorithm (machine learning model). The model would then predict which tracks in the dataset **best fits** the *imaginary tracks* generated by the ```linear vector interpolation``` method above. In other words, the models goal would be to predict Track ID's based on fake audio features.\n",
    "\n",
    "\n",
    "<img src=\"ml.jpg\" alt=\"Linear\" width=\"600\"/>\n",
    "\n",
    "\n",
    "As you can see, the output of the model is a list of tracks in the dataset, a playlist. Finally, the user would listen to these tracks sequentially and rate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rating of tracks \n",
    "When given a playlist, the user will rate each track according to the induced emotion he/she feels. The feedback will then be stored in the users profile. These preferences will then override the emotional parameters associated with the given track in the database, effectively generating a personalized dataset over time.  \n",
    "\n",
    "example: \n",
    "- The database might have track nr.10 rated as being \"sadness\" and \"nostalgic\".\n",
    "- However, if a user rates track nr.10 as being \"joyful_activation\" and \"power\", this rating will **override** the database's rating for this particular user. \n",
    "\n",
    "Let us rate some tracks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = {40: [\"amazement\"]}\n",
    "user1.set_track(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = {20: [\"tension\", \"sadness\"]}\n",
    "user1.set_track(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = {64: [\"joyful_activation\", \"tenderness\", \"nostalgia\"]}\n",
    "user1.set_track(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Profile\n",
    "Lets see what the user profile actually looks like. Note track 64, 20 and 40, which we recently logged with various emotional categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user1.user_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user1.log_out()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}